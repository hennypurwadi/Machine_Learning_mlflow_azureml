{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/hennypurwadi/mlflow_mlop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18352810",
   "metadata": {},
   "source": [
    "# Automate scheduled training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fed200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Every 720 hours do trigger_train() (last run: [never], next run: 2022-02-14 22:31:01)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile trigger_train.py\n",
    "\n",
    "def trigger_train(): \n",
    "   \n",
    "    import sklearn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report,accuracy_score\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    import datetime\n",
    "    import requests\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "       \n",
    "    filedf = 'fraud_detector.csv'\n",
    "    df= pd.read_csv(filedf)\n",
    "    X = df.drop(\"Category\",axis=1)\n",
    "    y = df.Category\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=40)\n",
    "\n",
    "    model= IsolationForest(n_estimators=100, max_samples=len(X_train),random_state=0, verbose=0)   \n",
    "    model.fit(X_train,y_train)\n",
    "    #model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "   \n",
    "    ypred= model.predict(X_test)\n",
    "    ypred[ypred == 1] = 0 #normal\n",
    "    ypred[ypred == -1] = 1 #possibly fraud \n",
    "    \n",
    "    #Freeze Model with joblib\n",
    "    filename_pkl = 'model.pkl'\n",
    "    joblib.dump(model, open(filename_pkl, 'wb'))\n",
    "    print(\"model.pkl saved\")\n",
    "    \n",
    "#Automate scheduled training    \n",
    "#mlflow.autolog({\"run_id\":\"749eb2eaf2a84e1992110481c7a7a7a9\"})  \n",
    "trigger_train()\n",
    "\n",
    "import schedule\n",
    "schedule.every(720).hours.do(trigger_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d50e49",
   "metadata": {},
   "source": [
    "# MLFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bb2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/15 16:35:37 INFO mlflow.tracking.fluent: Experiment with name 'evaluate_metric' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_metric (n_estimators=100.000000, max_samples=600.000000):\n",
      "  ACCURACY SCORE: 0.9766666666666667\n",
      "Save to: file:///C:/Users/HENNY/Documents/PYTHON/mlflow_project/mlruns/1/f0085241d45647bc857e46e724046884/artifacts\n",
      "\n",
      "\n",
      "evaluate_metric (n_estimators=110.000000, max_samples=630.000000):\n",
      "  ACCURACY SCORE: 0.9766666666666667\n",
      "Save to: file:///C:/Users/HENNY/Documents/PYTHON/mlflow_project/mlruns/1/e31ddbc4503841aeac3aade558bff921/artifacts\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "mlflow.tracking.get_tracking_uri()\n",
    "exp_name = \"evaluate_metric\"\n",
    "mlflow.set_experiment(exp_name)\n",
    "\n",
    "filedf = \"fraud_detector.csv\"\n",
    "df = pd.read_csv(filedf)  \n",
    "\n",
    "#Ttraining and testing dataset\n",
    "X = df.drop(\"Category\",axis=1)\n",
    "y = df.Category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=40)\n",
    "\n",
    "#Load model\n",
    "model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "#model= IsolationForest(n_estimators=100, max_samples=len(X_train),random_state=0, verbose=0)   \n",
    "#model.fit(X_train,y_train)\n",
    "\n",
    "ypred= model.predict(X_test)\n",
    "\n",
    "ypred[ypred == 1] = 0 #normal\n",
    "ypred[ypred == -1] = 1 #possibly fraud\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    # compute relevant metrics\n",
    "    acc_score = accuracy_score(y_test,ypred)\n",
    "    return acc_score\n",
    "\n",
    "def load_data(filedf):\n",
    "    df = pd.read_csv(filedf)  \n",
    "    X = df.drop(\"Category\",axis=1)\n",
    "    y = df.Category\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def main(n_estimators=40, max_samples=len(X_train)):\n",
    "    # train a model with given parameters\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Read csv file \n",
    "    filedf = \"fraud_detector.csv\"\n",
    "    train_x, train_y, test_x, test_y = load_data(filedf)\n",
    "\n",
    "    # Useful for multiple runs     \n",
    "    with mlflow.start_run():\n",
    "        # Load model\n",
    "        model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "                \n",
    "        ypred[ypred == 1] = 0 #normal\n",
    "        ypred[ypred == -1] = 1 #possibly fraud\n",
    "            \n",
    "        #Freeze Model with joblib\n",
    "        filename_pkl = 'model.pkl'\n",
    "        joblib.dump(model, open(filename_pkl, 'wb'))\n",
    "                \n",
    "        # Evaluate Metrics\n",
    "        predicted_qualities = model.predict(X_test)\n",
    "        (acc_score) = eval_metrics(y_test, predicted_qualities)\n",
    "\n",
    "        # Print out metrics\n",
    "        print(\"evaluate_metric (n_estimators=%f, max_samples=%f):\" % (n_estimators, max_samples))\n",
    "        print(\"  ACCURACY SCORE: %s\" % acc_score)\n",
    "       \n",
    "        # Log parameter, metrics, and model to MLflow\n",
    "        mlflow.log_param(key=\"n_estimators\", value=n_estimators)\n",
    "        mlflow.log_param(key=\"max_samples\", value=max_samples)\n",
    "        mlflow.log_metrics({\"accuracy score\":acc_score})\n",
    "        mlflow.log_artifact(filedf)\n",
    "        print(\"Save to: {}\".format(mlflow.get_artifact_uri()))\n",
    "        \n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "     for epoch in range(0, 3):\n",
    "        mlflow.log_metric(key=\"quality\", value=2*epoch, step=epoch)   \n",
    "        \n",
    "main(100,600)\n",
    "print('\\n')\n",
    "main(110,630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796b9aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Experiment: artifact_location='file:///C:/Users/HENNY/Documents/PYTHON/mlflow_project/mlruns/0', experiment_id='0', lifecycle_stage='active', name='Default', tags={}>, <Experiment: artifact_location='file:///C:/Users/HENNY/Documents/PYTHON/mlflow_project/mlruns/1', experiment_id='1', lifecycle_stage='active', name='evaluate_metric', tags={}>]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiments = client.list_experiments() # returns a list of mlflow.entities.Experiment\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c544a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Run: data=<RunData: metrics={'accuracy score': 0.9766666666666667}, params={'max_samples': '630', 'n_estimators': '110'}, tags={'mlflow.log-model.history': '[{\"run_id\": \"e31ddbc4503841aeac3aade558bff921\", '\n",
      "                             '\"artifact_path\": \"model\", \"utc_time_created\": '\n",
      "                             '\"2022-01-15 09:35:45.038400\", \"flavors\": '\n",
      "                             '{\"python_function\": {\"model_path\": \"model.pkl\", '\n",
      "                             '\"loader_module\": \"mlflow.sklearn\", '\n",
      "                             '\"python_version\": \"3.8.8\", \"env\": \"conda.yaml\"}, '\n",
      "                             '\"sklearn\": {\"pickled_model\": \"model.pkl\", '\n",
      "                             '\"sklearn_version\": \"0.23.2\", '\n",
      "                             '\"serialization_format\": \"cloudpickle\"}}}]',\n",
      " 'mlflow.source.name': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
      " 'mlflow.source.type': 'LOCAL',\n",
      " 'mlflow.user': 'HENNY'}>, info=<RunInfo: artifact_uri='file:///C:/Users/HENNY/Documents/PYTHON/mlflow_project/mlruns/1/e31ddbc4503841aeac3aade558bff921/artifacts', end_time=1642239349412, experiment_id='1', lifecycle_stage='active', run_id='e31ddbc4503841aeac3aade558bff921', run_uuid='e31ddbc4503841aeac3aade558bff921', start_time=1642239344917, status='FINISHED', user_id='HENNY'>>\n"
     ]
    }
   ],
   "source": [
    "# get the run\n",
    "_run = client.get_run(run_id=\"e31ddbc4503841aeac3aade558bff921\")\n",
    "print(_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73110a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method MlflowClient.set_tag of <mlflow.tracking.client.MlflowClient object at 0x0000026BAC227CA0>>\n",
      "\n",
      "\n",
      "15-01-2022 (16:45:35.943287)\n"
     ]
    }
   ],
   "source": [
    "# add a tag to the run\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y (%H:%M:%S.%f)\")\n",
    "client.set_tag(_run.info.run_id, \"deployed\", dt)\n",
    "print(client.set_tag)\n",
    "print('\\n')\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904017e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/16 13:31:49 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/01/16 13:31:49 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '7613e491506842e2aca36b471acaeb48', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Every 720 hours do trigger_train() (last run: [never], next run: 2022-02-15 13:31:54)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile trigger_train.py\n",
    "\n",
    "def trigger_train(): \n",
    "   \n",
    "    import sklearn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report,accuracy_score\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import joblib\n",
    "    import datetime\n",
    "    import requests\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "       \n",
    "    filedf = 'fraud_detector.csv'\n",
    "    df= pd.read_csv(filedf)\n",
    "    X = df.drop(\"Category\",axis=1)\n",
    "    y = df.Category\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=40)\n",
    "\n",
    "    model= IsolationForest(n_estimators=100, max_samples=len(X_train),random_state=0, verbose=0)   \n",
    "    model.fit(X_train,y_train)\n",
    "    #model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "   \n",
    "    ypred= model.predict(X_test)\n",
    "    ypred[ypred == 1] = 0 #normal\n",
    "    ypred[ypred == -1] = 1 #possibly fraud \n",
    "    \n",
    "    #Freeze Model with joblib\n",
    "    filename_pkl = 'model.pkl'\n",
    "    joblib.dump(model, open(filename_pkl, 'wb'))\n",
    "    print(\"model.pkl saved\")\n",
    "    \n",
    "#Automate scheduled training mlflow Run_id    \n",
    "mlflow.autolog({\"run_id\":\"e31ddbc4503841aeac3aade558bff921\"})  \n",
    "trigger_train()\n",
    "\n",
    "import schedule\n",
    "schedule.every(720).hours.do(trigger_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa83d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "exp_name = \"evaluate_metric\"\n",
    "mlflow.set_experiment(exp_name)\n",
    "\n",
    "filedf = \"fraud_detector.csv\"\n",
    "df = pd.read_csv(filedf)  \n",
    "\n",
    "#Ttraining and testing dataset\n",
    "X = df.drop(\"Category\",axis=1)\n",
    "y = df.Category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=40)\n",
    "\n",
    "model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "#model= IsolationForest(n_estimators=100, max_samples=len(X_train),random_state=0, verbose=0)   \n",
    "#model.fit(X_train,y_train)\n",
    "\n",
    "ypred= model.predict(X_test)\n",
    "\n",
    "ypred[ypred == 1] = 0 #normal\n",
    "ypred[ypred == -1] = 1 #possibly fraud\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    # compute relevant metrics\n",
    "    acc_score = accuracy_score(y_test,ypred)\n",
    "    return acc_score\n",
    "\n",
    "def load_data(filedf):\n",
    "    df = pd.read_csv(filedf)  \n",
    "    X = df.drop(\"Category\",axis=1)\n",
    "    y = df.Category\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def main(n_estimators=40, max_samples=len(X_train)):\n",
    "    # train a model with given parameters\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Read csv file \n",
    "    filedf = \"fraud_detector.csv\"\n",
    "    train_x, train_y, test_x, test_y = load_data(filedf)\n",
    "\n",
    "    # Useful for multiple runs     \n",
    "    with mlflow.start_run():\n",
    "        # Execute \n",
    "        model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "        #model= IsolationForest(n_estimators=n_estimators, max_samples=max_samples,random_state=0, verbose=0)   \n",
    "        #model.fit(X_train,y_train)\n",
    "        \n",
    "        ypred[ypred == 1] = 0 #normal\n",
    "        ypred[ypred == -1] = 1 #possibly fraud\n",
    "            \n",
    "        #Freeze Model with joblib\n",
    "        filename_pkl = 'model.pkl'\n",
    "        joblib.dump(model, open(filename_pkl, 'wb'))\n",
    "        print(\"model.pkl saved\")\n",
    "        \n",
    "        # Evaluate Metrics\n",
    "        predicted_qualities = model.predict(X_test)\n",
    "        (acc_score) = eval_metrics(y_test, predicted_qualities)\n",
    "\n",
    "        # Print out metrics\n",
    "        print(\"evaluate_metric (n_estimators=%f, max_samples=%f):\" % (n_estimators, max_samples))\n",
    "        print(\"  ACCURACY SCORE: %s\" % acc_score)\n",
    "       \n",
    "        # Log parameter, metrics, and model to MLflow\n",
    "        mlflow.log_param(key=\"n_estimators\", value=n_estimators)\n",
    "        mlflow.log_param(key=\"max_samples\", value=max_samples)\n",
    "        mlflow.log_metrics({\"accuracy score\":acc_score})\n",
    "        mlflow.log_artifact(filedf)\n",
    "        print(\"Save to: {}\".format(mlflow.get_artifact_uri()))\n",
    "        \n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "     for epoch in range(0, 3):\n",
    "        mlflow.log_metric(key=\"quality\", value=2*epoch, step=epoch)        \n",
    "        \n",
    "main(107, 680)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1c0f6",
   "metadata": {},
   "source": [
    "#### From terminal type \n",
    "(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> mlflow ui\n",
    "\n",
    "(envi1) (base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> mlflow ui\n",
    "INFO:waitress:Serving on http://127.0.0.1:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schedule.every(720).hours.do(trigger_train)  \n",
    "#schedule.every(10).seconds.do(trigger_train)\n",
    "#schedule.every(15).minutes.do(trigger_train)\n",
    "#schedule.every().day.at('09:01').do(trigger_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9536377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163f573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/status\")\n",
    "def status():\n",
    "    return \"success\"\n",
    "\n",
    "@app.route(\"/\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "    A1 = request.args.get(\"A1\", None)\n",
    "    A2 = request.args.get(\"A2\", None)\n",
    "\n",
    "    #request_value = request.get_json()\n",
    "\n",
    "    #A1 = int(request_value[\"A1\"])\n",
    "    #A2 = int(request_value[\"A2\"])\n",
    "\n",
    "    if A1 != None:\n",
    "        y_new = predict(A1, A2)\n",
    "    else:\n",
    "        y_new = \"\"\n",
    "\n",
    "    write(A1, A2, y_new)\n",
    "    return (\n",
    "        \"\"\"<form action=\"\" method=\"get\">\n",
    "                A1 input: <input type=\"text\" name=\"A1\">\n",
    "                A2 input: <input type=\"text\" name=\"A2\">\n",
    "                <input type=\"submit\" value=\"A1 & A2 input for Predict Fraud or Not\">\n",
    "            </form>\"\"\"\n",
    "\n",
    "        + \"y_new: \"\n",
    "        + str(y_new)\n",
    "    )\n",
    "\n",
    "@app.route(\"/json\", methods=['GET', 'POST'])\n",
    "def jsonify():\n",
    "    request_value = request.get_json()\n",
    "    return request_value\n",
    "\n",
    "def write(A1, A2, y_new):\n",
    "    filedf = \"fraud_detector.csv\"\n",
    "    # write new data into csv\n",
    "    with open(filedf, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([A1, A2, y_new])\n",
    "        print(\"file written\")\n",
    "\n",
    "def predict(A1, A2):\n",
    "    \"\"\"Predict Fraud or Not Fraud.\"\"\"\n",
    "    print(\"predicting\")\n",
    "\n",
    "    model = joblib.load(open(\"model.pkl\", 'rb'))\n",
    "    X_new = np.array([A1, A2]).reshape(1, -1)\n",
    "    y_new = model.predict(X_new)\n",
    "\n",
    "    y_new[y_new == 1] = 0  # normal\n",
    "    y_new[y_new == -1] = 1  # possibly fraud\n",
    "\n",
    "    y_new = (int(y_new))\n",
    "    return y_new\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=int(\"5000\"), debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8952475",
   "metadata": {},
   "source": [
    "#### Create image and Docker Container  \n",
    "(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> pip install --user virtualenv\n",
    "\n",
    "(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> python -m venv envi1\n",
    "\n",
    "(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project>.\\envi1\\Scripts\\activate\n",
    "\n",
    "(envi1)(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> pip install -r requirements.txt\n",
    "\n",
    "(envi1)(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> docker build -t image01 .\n",
    "\n",
    "(envi1)(base) PS C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project> docker run --name container01 -p 5000:5000 image01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa460865",
   "metadata": {},
   "source": [
    "# AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6ac37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying KeyVault with name ws1keyvault5baf6f8ab8d64.\n",
      "Deploying StorageAccount with name ws1storage4dbfefc102d446.\n",
      "Deploying AppInsights with name ws1insightsde9d189167484.\n",
      "Deployed AppInsights with name ws1insightsde9d189167484. Took 4.48 seconds.\n",
      "Deploying Workspace with name ws1.\n",
      "Deployed KeyVault with name ws1keyvault5baf6f8ab8d64. Took 22.98 seconds.\n",
      "Deployed StorageAccount with name ws1storage4dbfefc102d446. Took 29.27 seconds.\n",
      "Deployed Workspace with name ws1. Took 38.88 seconds.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import azure\n",
    "import azureml.core\n",
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "import mlflow.azureml\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "AZ_SUBSCRIPTION_ID='b667c281-58f0-47a7-899b-63ea6cd8b7e8' #azure-subscription-id\n",
    "ws = Workspace.create(name='ws1',subscription_id=AZ_SUBSCRIPTION_ID, \n",
    "                      resource_group='res1',create_resource_group=True,location='southeastasia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f769dcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_metric (n_estimators=120.000000, max_samples=700.000000):\n",
      "  ACCURACY SCORE: 0.9833333333333333\n",
      "Save to: azureml://experiments/eval_metric/runs/04709530-c2be-473d-9d95-0acd270c6cd0/artifacts\n"
     ]
    }
   ],
   "source": [
    "#%%writefile score.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "from flask import Flask, flash, request, redirect, url_for\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import azure\n",
    "import azureml.core\n",
    "from datetime import datetime\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "import mlflow.azureml\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core import Environment\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "mlflow.end_run() #Close previous Run\n",
    "ws.write_config(path=\".\", file_name=\"ws_config.json\")\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "#mlflow.create_experiment(exp_name)\n",
    "exp_nameB = \"eval_metric\"\n",
    "mlflow.set_experiment(exp_nameB)\n",
    "#mlflow_run = mlflow.start_run()\n",
    "my_uri = ws.get_mlflow_tracking_uri()\n",
    "\n",
    "filedf = \"fraud_detector.csv\"\n",
    "df = pd.read_csv(filedf)  \n",
    "\n",
    "#Ttraining and testing dataset\n",
    "X = df.drop(\"Category\",axis=1)\n",
    "y = df.Category\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)\n",
    "\n",
    "model= IsolationForest(n_estimators=100, max_samples=len(X_train),random_state=0, verbose=0)   \n",
    "model.fit(X_train,y_train)\n",
    "ypred= model.predict(X_test)\n",
    "\n",
    "ypred[ypred == 1] = 0 #normal\n",
    "ypred[ypred == -1] = 1 #possibly fraud\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    # compute relevant metrics\n",
    "    acc_score = accuracy_score(y_test,ypred)\n",
    "    return acc_score\n",
    "\n",
    "def load_data(filedf):\n",
    "    df = pd.read_csv(filedf)  \n",
    "    X = df.drop(\"Category\",axis=1)\n",
    "    y = df.Category\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train(n_estimators=40, max_samples=len(X_train)):\n",
    "    # train a model with given parameters\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Read csv file \n",
    "    filedf = \"fraud_detector.csv\"\n",
    "    train_x, train_y, test_x, test_y = load_data(filedf)\n",
    "\n",
    "    # Useful for multiple runs     \n",
    "    with mlflow.start_run():\n",
    "        # Execute \n",
    "        model= IsolationForest(n_estimators=n_estimators, max_samples=max_samples,random_state=0, verbose=0)   \n",
    "        model.fit(X_train,y_train)\n",
    "        \n",
    "        ypred[ypred == 1] = 0 #normal\n",
    "        ypred[ypred == -1] = 1 #possibly fraud\n",
    "        \n",
    "        # Evaluate Metrics\n",
    "        predicted_qualities = model.predict(X_test)\n",
    "        (acc_score) = eval_metrics(y_test, predicted_qualities)\n",
    "\n",
    "        # Print out metrics\n",
    "        print(\"evaluate_metric (n_estimators=%f, max_samples=%f):\" % (n_estimators, max_samples))\n",
    "        print(\"  ACCURACY SCORE: %s\" % acc_score)\n",
    "       \n",
    "        # Log parameter, metrics, and model to MLflow\n",
    "        mlflow.log_param(key=\"n_estimators\", value=n_estimators)\n",
    "        mlflow.log_param(key=\"max_samples\", value=max_samples)\n",
    "        mlflow.log_metrics({\"accuracy score\":acc_score})\n",
    "        mlflow.log_artifact(filedf)\n",
    "        print(\"Save to: {}\".format(mlflow.get_artifact_uri()))\n",
    "        \n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "     for epoch in range(0, 3):\n",
    "        mlflow.log_metric(key=\"quality\", value=2*epoch, step=epoch)        \n",
    "        \n",
    "train(120, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694f9782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate_metric (n_estimators=200.000000, max_samples=600.000000):\n",
      "  ACCURACY SCORE: 0.9833333333333333\n",
      "Save to: azureml://experiments/eval_metric/runs/32f78059-caba-45db-a885-7b11f3506623/artifacts\n"
     ]
    }
   ],
   "source": [
    "train(200, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca954411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37b89e16-4cd3-435d-b823-d5eecd040820\n",
      "bdb569bd-d092-481d-a34c-8d1b39eb6c9f\n",
      "32f78059-caba-45db-a885-7b11f3506623\n",
      "04709530-c2be-473d-9d95-0acd270c6cd0\n",
      "8453c18a-f1fa-460c-880d-8ce1ae7ce07b\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run() #End previous Run\n",
    "mlflow_run = mlflow.start_run() #start new Run\n",
    "\n",
    "client = MlflowClient()\n",
    "finished_mlflow_run = MlflowClient().get_run(mlflow_run.info.run_id)\n",
    "\n",
    "exp = Experiment(workspace=ws, name=exp_nameB)\n",
    "list_experiments = exp.list(ws)\n",
    "\n",
    "list_runs = exp.get_runs()\n",
    "for run in list_runs:\n",
    "    print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65165c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mlflow.user': 'HENNY', 'mlflow.source.name': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', 'mlflow.source.type': 'LOCAL', 'mlflow.rootRunId': '37b89e16-4cd3-435d-b823-d5eecd040820'}\n"
     ]
    }
   ],
   "source": [
    "metrics = finished_mlflow_run.data.metrics\n",
    "tags = finished_mlflow_run.data.tags\n",
    "params = finished_mlflow_run.data.params\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8c9098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model fraud_detect\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model.register(model_path = \"model.pkl\",\n",
    "                       model_name = \"fraud_detect\",\n",
    "                       tags = {\"key\": \"1\"},\n",
    "                       description = \"fraud Prediction\", \n",
    "                       workspace = ws,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da5a630e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HENNY\\\\Documents\\\\PYTHON\\\\mlflow_project\\\\model.pkl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import model\n",
    "model = Model(workspace=ws, name=\"fraud_detect\")\n",
    "model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "#model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26785e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraud_detect\tfraud_detect:1\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "azureml.core.runconfig.RunConfiguration"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.name, model.id, model.version, sep='\\t')\n",
    "azureml.core.compute.ComputeTarget\n",
    "azureml.core.runconfig.RunConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfaa7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.compute import AmlCompute\n",
    "list_vms = AmlCompute.supported_vmsizes(workspace=ws)\n",
    "\n",
    "compute_config = RunConfiguration()\n",
    "#compute_config.target = \"amlcompute\"\n",
    "#compute_config.amlcompute.vm_size = \"STANDARD_D1_V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602b7ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # retrieve the path to the model file using the model name\n",
    "    model_path = Model.get_model_path('fraud_detect')\n",
    "    #model_path = \"model.pkl\"\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    ypred = model.predict(data)\n",
    "    return json.dumps(ypred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dfc7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", runtime=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503b876",
   "metadata": {},
   "source": [
    "### Inference Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d848864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferenceConfig(entry_script=score.py, runtime=None, conda_file=None, extra_docker_file_steps=None, source_directory=C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project, enable_gpu=None, base_image=None, base_image_registry=<azureml.core.container_registry.ContainerRegistry object at 0x000001C23506C2B0>)\n",
      "<class 'azureml.core.model.InferenceConfig'>\n",
      "<azureml.core.image.container.ContainerImageConfig object at 0x000001C235147490>\n"
     ]
    }
   ],
   "source": [
    "env=ws.environments['AzureML-mlflow-ubuntu18.04-py37-cpu-inference']\n",
    "\n",
    "dummy_inference_config = InferenceConfig(environment=env, source_directory='.', entry_script=\"score.py\")\n",
    "print(dummy_inference_config)\n",
    "print(InferenceConfig)\n",
    "print(image_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9cfff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.core.webservice.aci.AciServiceDeploymentConfiguration at 0x1c235ad2580>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "# Create a deployment config\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1, auth_enabled=True)\n",
    "aci_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ede1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InferenceConfig(entry_script=score.py, runtime=None, conda_file=None, extra_docker_file_steps=None, source_directory=C:\\Users\\HENNY\\Documents\\PYTHON\\mlflow_project, enable_gpu=None, base_image=None, base_image_registry=<azureml.core.container_registry.ContainerRegistry object at 0x00000219AF2FA190>)\n",
      "<class 'azureml.core.model.InferenceConfig'>\n",
      "<azureml.core.image.container.ContainerImageConfig object at 0x00000219AF2FAFA0>\n"
     ]
    }
   ],
   "source": [
    "# create environment for the deploy\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# get a curated environment\n",
    "#env=ws.environments['AzureML-mlflow-ubuntu18.04-py37-cpu-inference']\n",
    "\n",
    "env = Environment.get(workspace=ws, name=\"AzureML-mlflow-ubuntu18.04-py37-cpu-inference\",version=1)\n",
    "env.inferencing_stack_version='latest'\n",
    "\n",
    "# create deployment config i.e. compute resources\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1,memory_gb=1,\n",
    "                        tags={\"data\": \"MNIST\", \"method\": \"sklearn\"},description=\"Predict fraud\")\n",
    "\n",
    "dummy_inference_config = InferenceConfig(environment=env, source_directory='.', entry_script=\"score.py\")\n",
    "print(dummy_inference_config)\n",
    "print(InferenceConfig)\n",
    "print(image_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3786d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<azureml.core.script_run_config.ScriptRunConfig object at 0x00000219AE9A7370>\n"
     ]
    }
   ],
   "source": [
    "src = ScriptRunConfig(source_directory='.', script='score.py', environment=env)\n",
    "print(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029a3b0",
   "metadata": {},
   "source": [
    "## Custom environment yaml.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d878c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\n",
      "\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "\n",
      "  - azureml-defaults\n",
      "\n",
      "  - pip==21.0.1\n",
      "  - azureml.core\n",
      "- ''\n",
      "- scikit-learn\n",
      "- mlflow\n",
      "- numpy\n",
      "- pandas\n",
      "- joblib\n",
      "- flask == 2.0.1\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package('')\n",
    "\n",
    "myenv.add_pip_package('pip==21.0.1')\n",
    "myenv.add_pip_package('azureml.core')\n",
    "myenv.add_conda_package(\"scikit-learn\")\n",
    "myenv.add_conda_package(\"mlflow\")\n",
    "myenv.add_conda_package('numpy')\n",
    "myenv.add_conda_package(\"pandas\")\n",
    "myenv.add_conda_package(\"joblib\")\n",
    "myenv.add_conda_package(\"python==3.6.2\")\n",
    "myenv.add_conda_package(\"flask == 2.0.1\")\n",
    "\n",
    "with open(\"myenv.Yaml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "with open(\"myenv.Yaml\",\"r\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "myenv = Environment.from_conda_specification(name='myenv', file_path='myenv.yaml')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
